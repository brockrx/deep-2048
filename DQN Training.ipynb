{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffc564f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from logic import Game\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import copy\n",
    "import concurrent.futures\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "831d10e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNTrainer():\n",
    "    def __init__(\n",
    "        self,\n",
    "        env,\n",
    "        agent,\n",
    "        epsilon = 1,\n",
    "        epsilon_decay = 0.999,\n",
    "        epsilon_min = 0.3,\n",
    "        replay_size = 1e6,\n",
    "        batch_size = 2048,\n",
    "        gamma = 0.99,\n",
    "        alpha = 0.001,\n",
    "    ):\n",
    "        \n",
    "        self.agent = agent\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.env = env\n",
    "        self.batch_size = 32\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.train_initialized = False\n",
    "        \n",
    "        self.memory = ReplayMemory(replay_size, self.agent.input_size)\n",
    "        self.initialize_policy()\n",
    "        \n",
    "    def initialize_policy(self):\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.agent.policy.parameters(), lr=self.alpha)\n",
    "        \n",
    "        \n",
    "    def train(self, episodes, eval_interval=100):\n",
    "        if not self.train_initialized:\n",
    "            self.score_list = []\n",
    "            self.eval_list = []\n",
    "            mean_score = self.evaluate(100)\n",
    "            self.eval_list.append(mean_score)\n",
    "            print(f'Iteration {0}: {mean_score}')\n",
    "            self.train_initialized = True\n",
    "        \n",
    "        for e in range(1,episodes+1):\n",
    "            s = self.env.reset().copy()\n",
    "            score = 0\n",
    "            done = 0\n",
    "            while done == 0:\n",
    "                a = self.agent.get_action(s, epsilon=self.epsilon, training=True)\n",
    "                s1, r, done, _ = self.env.step(a)\n",
    "                score += r\n",
    "                self.memory.add_transition((s, a, r, s1, int(done)))\n",
    "                self.update()\n",
    "                s = s1\n",
    "            self.epsilon = max(self.epsilon_decay*self.epsilon, self.epsilon_min)\n",
    "            self.score_list.append(score)\n",
    "            if e%eval_interval == 0 or e == 0:\n",
    "                mean_score = self.evaluate(100)\n",
    "                self.eval_list.append(mean_score)\n",
    "                print(f'Iteration {e}: {mean_score}')\n",
    "                \n",
    "                \n",
    "    def update(self):\n",
    "        \n",
    "        # Extract values from data array\n",
    "        n_samples = min(self.memory.replay_size, self.batch_size)\n",
    "        s, a, r, s1, t = self.memory.get_replay_sample(n_samples)\n",
    "\n",
    "        s = torch.tensor(s).float()\n",
    "        a = torch.tensor(np.expand_dims(a, 1)).to(torch.int64)\n",
    "        r = torch.tensor(np.expand_dims(r, axis=1))\n",
    "        s1 = torch.tensor(s1).float()\n",
    "        non_term = torch.tensor(np.expand_dims(1 - t, axis=1))\n",
    "\n",
    "        # Calculate Target\n",
    "        with torch.no_grad():\n",
    "            v1 = torch.max(self.agent.policy.forward(s1), 1, keepdim=True)[0]\n",
    "        target = r + (self.gamma * v1 * non_term)\n",
    "        target = target.float()\n",
    "\n",
    "        # Calculate prediction\n",
    "        self.optimizer.zero_grad()\n",
    "        pred = self.agent.policy.forward(s, train=True).gather(dim=1, index=a).float()\n",
    "\n",
    "        # Perform gradient descent step\n",
    "        loss = self.loss_fn(pred, target)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        delta = loss.item()\n",
    "\n",
    "            \n",
    "    def evaluate(self, episodes):\n",
    "        score_arr = np.empty(episodes)\n",
    "        for e in range(episodes):\n",
    "            s = self.env.reset()\n",
    "            score = 0\n",
    "            done = False\n",
    "            while not done:\n",
    "                legal_moves = self.env.get_valid_moves()\n",
    "                illegal_moves = [action for action in range(4) if action not in legal_moves]\n",
    "                a = self.agent.get_action(s, training=False, illegal_actions=illegal_moves)\n",
    "                s1, r, done, _ = self.env.step(a)\n",
    "                score += r\n",
    "                s = s1\n",
    "            score_arr[e] = score\n",
    "        return np.mean(score_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efba164f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(\n",
    "        self,\n",
    "        layers = [256, 256],\n",
    "        action_size = 4,\n",
    "        state_size = 4\n",
    "        ):\n",
    "        \n",
    "        self.layers = layers\n",
    "        self.output_size = action_size\n",
    "        self.input_size = state_size\n",
    "        \n",
    "        self.policy = CNN()         \n",
    "        \n",
    "            \n",
    "    def get_action(self, state, epsilon=0, training=True, illegal_actions=[]):\n",
    "        val = np.random.random()\n",
    "        if val >= epsilon and training:\n",
    "            with torch.no_grad():\n",
    "                action = torch.argmax(self.policy.forward(np.expand_dims(state, 0))[0,:]).item()\n",
    "        elif not training:\n",
    "            action_values = self.policy.forward(np.expand_dims(state, 0))[0,:]\n",
    "            min_value = torch.min(action_values)\n",
    "            for action in illegal_actions:\n",
    "                action_values[action] = min_value - 1\n",
    "            action = torch.argmax(action_values).item()\n",
    "        else:\n",
    "            action = np.random.randint(0, high=self.output_size)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58422c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, layer_dims):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.layer_dims = layer_dims\n",
    "        current_dim = input_dim\n",
    "        \n",
    "        self.create_model()\n",
    "        \n",
    "        \n",
    "    def create_model(self):            \n",
    "        self.module_list = nn.ModuleList()\n",
    "        input_dim = self.input_dim\n",
    "        for layer_dim in self.layer_dims:\n",
    "            self.module_list.append(nn.Linear(input_dim, layer_dim))\n",
    "            self.module_list.append(nn.ReLU())\n",
    "            self.module_list.append(nn.BatchNorm1d(layer_dim))\n",
    "            input_dim = layer_dim\n",
    "        self.module_list.append(nn.Linear(input_dim, self.output_dim))\n",
    "        print(self.module_list)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, train=False):\n",
    "        if type(x) is not torch.Tensor:\n",
    "            x = torch.tensor(x)\n",
    "        if not train:\n",
    "            self.module_list.eval()\n",
    "        else:\n",
    "            self.module_list.train()\n",
    "        out = x.float()\n",
    "        for module in self.module_list:\n",
    "            out = module(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def get_weights(self):\n",
    "        weights = []\n",
    "        for param in self.parameters():\n",
    "            weights.append(param.detach().numpy().flatten())\n",
    "        return np.concatenate(weights)\n",
    "            \n",
    "        \n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_dim=4, output_dim=4, conv_dim=256, dense_dims=[256, 256]):\n",
    "        super(CNN, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.conv_dim = conv_dim\n",
    "        self.dense_dims = dense_dims\n",
    "        \n",
    "        self.create_model()\n",
    "        \n",
    "        \n",
    "    def create_model(self):            \n",
    "        self.conv1 = nn.Conv2d(1, self.conv_dim, (1,4))\n",
    "        self.conv2 = nn.Conv2d(1, self.conv_dim, (4,1))\n",
    "        self.conv3 = nn.Conv2d(1, self.conv_dim, (2,2))\n",
    "        self.module_list = nn.ModuleList()\n",
    "        input_dim = self.conv_dim * (4 + 4 + 9)\n",
    "        self.module_list.append(nn.ReLU())\n",
    "        self.module_list.append(nn.BatchNorm1d(input_dim))\n",
    "        for layer_dim in self.dense_dims:\n",
    "            self.module_list.append(nn.Linear(input_dim, layer_dim))\n",
    "            self.module_list.append(nn.ReLU())\n",
    "            self.module_list.append(nn.BatchNorm1d(layer_dim))\n",
    "            input_dim = layer_dim\n",
    "        self.module_list.append(nn.Linear(input_dim, self.output_dim))\n",
    "        print(self.module_list)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, train=False):\n",
    "        if type(x) is not torch.Tensor:\n",
    "            x = torch.tensor(x)\n",
    "        if not train:\n",
    "            self.module_list.eval()\n",
    "        else:\n",
    "            self.module_list.train()\n",
    "        x = x.float()\n",
    "        m = x.size()[0]\n",
    "        out1 = self.conv1(x).reshape(m,-1)\n",
    "        out2 = self.conv2(x).reshape(m,-1)\n",
    "        out3 = self.conv3(x).reshape(m,-1)\n",
    "        out = torch.cat([out1, out2, out3], 1)\n",
    "        for module in self.module_list:\n",
    "            out = module(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def get_weights(self):\n",
    "        weights = []\n",
    "        for param in self.parameters():\n",
    "            weights.append(param.detach().numpy().flatten())\n",
    "        return np.concatenate(weights)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "896e91d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    \n",
    "    def __init__(self, replay_size, input_size):\n",
    "        self.replay_size = int(replay_size)\n",
    "        self.input_size = input_size\n",
    "        self.initialize_replay_memory()\n",
    "        \n",
    "    \n",
    "    def initialize_replay_memory(self):\n",
    "        self.states = np.zeros((self.replay_size, 1, self.input_size,self.input_size))\n",
    "        self.actions = np.zeros(self.replay_size)\n",
    "        self.rewards = np.zeros(self.replay_size)\n",
    "        self.next_states = np.zeros((self.replay_size, 1, self.input_size, self.input_size))\n",
    "        self.terminals = np.zeros(self.replay_size)\n",
    "        \n",
    "        self.replay_index = 0\n",
    "        self.replay_full = False\n",
    "        self.replay_max = 1\n",
    "        \n",
    "    \n",
    "    def add_transition(self, transition):\n",
    "        i = self.replay_index\n",
    "        \n",
    "        self.states[i,:,:,:] = transition[0]\n",
    "        self.actions[i] = transition[1]\n",
    "        self.rewards[i] = transition[2]\n",
    "        self.next_states[i,:,:,:] = transition[3]\n",
    "        self.terminals[i] = transition[4]\n",
    "        \n",
    "        self.replay_index += 1\n",
    "        self.replay_max = min(self.replay_max+1, self.replay_size)\n",
    "        if self.replay_index == self.replay_size:\n",
    "            self.replay_index = 0\n",
    "            self.replay_full = True\n",
    "            \n",
    "        \n",
    "    def get_replay_sample(self, samples, index=None):\n",
    "        if index is None:\n",
    "            index = np.random.randint(0,high=self.replay_max, size=samples)\n",
    "        states = self.states[index,:,:,:]\n",
    "        actions = self.actions[index]\n",
    "        rewards = self.rewards[index]\n",
    "        next_states = self.next_states[index,:,:,:]\n",
    "        terminals = self.terminals[index]\n",
    "        return (states, actions, rewards, next_states, terminals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "471417f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestEnv1():\n",
    "    \n",
    "    def __init__(self, size=5, slip_prob=0):\n",
    "        self.size = size\n",
    "        self.state = np.zeros(size, dtype=int)\n",
    "        self.slip_prob = slip_prob\n",
    "        \n",
    "    def reset(self):\n",
    "        index = np.random.randint(0, high=self.size)\n",
    "        self.state = np.zeros(self.size, dtype=int)\n",
    "        self.state[index] = 1\n",
    "        self.score = 0\n",
    "        return self.state.copy()\n",
    "        \n",
    "    def step(self, action):\n",
    "        direction = -1 if action == 0 else 1\n",
    "        if random.random() < self.slip_prob:\n",
    "            direction *= -1\n",
    "        current_index = np.argmax(self.state)\n",
    "        \n",
    "        if current_index == 4 and direction == 1:\n",
    "            return np.zeros(self.size), 1, 1, None\n",
    "        if current_index == 0 and direction == -1:\n",
    "            return np.zeros(self.size), -1, 1, None\n",
    "        \n",
    "        self.state[current_index] = 0\n",
    "        self.state[current_index + direction] = 1\n",
    "        \n",
    "        return copy.deepcopy(self.state), 0, 0, None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2c63d3e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModuleList(\n",
      "  (0): ReLU()\n",
      "  (1): BatchNorm1d(4352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): Linear(in_features=4352, out_features=256, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (5): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (6): ReLU()\n",
      "  (7): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (8): Linear(in_features=256, out_features=4, bias=True)\n",
      ")\n",
      "Iteration 0: 1341.68\n",
      "Iteration 50: 988.72\n",
      "Iteration 100: 1426.72\n",
      "Iteration 150: 1312.72\n",
      "Iteration 200: 1253.96\n",
      "Iteration 250: 700.24\n",
      "Iteration 300: 806.12\n",
      "Iteration 350: 623.0\n",
      "Iteration 400: 650.6\n",
      "Iteration 450: 654.88\n",
      "Iteration 500: 783.68\n",
      "Iteration 550: 678.96\n",
      "Iteration 600: 676.52\n",
      "Iteration 650: 687.36\n",
      "Iteration 700: 642.0\n",
      "Iteration 750: 615.48\n",
      "Iteration 800: 656.2\n",
      "Iteration 850: 683.24\n",
      "Iteration 900: 696.76\n",
      "Iteration 950: 630.92\n",
      "Iteration 1000: 731.48\n",
      "ModuleList(\n",
      "  (0): ReLU()\n",
      "  (1): BatchNorm1d(4352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): Linear(in_features=4352, out_features=256, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (5): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (6): ReLU()\n",
      "  (7): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (8): Linear(in_features=256, out_features=4, bias=True)\n",
      ")\n",
      "Iteration 0: 1330.68\n",
      "Iteration 50: 1290.92\n",
      "Iteration 100: 1386.56\n",
      "Iteration 150: 1392.72\n",
      "Iteration 200: 1313.04\n",
      "Iteration 250: 1475.08\n",
      "Iteration 300: 1295.24\n",
      "Iteration 350: 1194.16\n",
      "Iteration 400: 597.36\n",
      "Iteration 450: 1281.36\n",
      "Iteration 500: 755.8\n",
      "Iteration 550: 698.2\n",
      "Iteration 600: 716.36\n",
      "Iteration 650: 681.6\n",
      "Iteration 700: 723.24\n",
      "Iteration 750: 740.92\n",
      "Iteration 800: 684.32\n",
      "Iteration 850: 737.44\n",
      "Iteration 900: 651.68\n",
      "Iteration 950: 682.08\n",
      "Iteration 1000: 615.36\n",
      "ModuleList(\n",
      "  (0): ReLU()\n",
      "  (1): BatchNorm1d(4352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): Linear(in_features=4352, out_features=256, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (5): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (6): ReLU()\n",
      "  (7): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (8): Linear(in_features=256, out_features=4, bias=True)\n",
      ")\n",
      "Iteration 0: 1332.32\n",
      "Iteration 50: 910.76\n",
      "Iteration 100: 871.16\n",
      "Iteration 150: 819.68\n",
      "Iteration 200: 792.8\n",
      "Iteration 250: 987.96\n",
      "Iteration 300: 1024.4\n",
      "Iteration 350: 1234.72\n",
      "Iteration 400: 1301.64\n",
      "Iteration 450: 1466.36\n",
      "Iteration 500: 1302.16\n",
      "Iteration 550: 1395.88\n",
      "Iteration 600: 1482.4\n",
      "Iteration 650: 747.08\n",
      "Iteration 700: 1090.2\n",
      "Iteration 750: 1065.56\n",
      "Iteration 800: 764.52\n",
      "Iteration 850: 625.96\n",
      "Iteration 900: 653.52\n",
      "Iteration 950: 707.4\n",
      "Iteration 1000: 684.12\n",
      "ModuleList(\n",
      "  (0): ReLU()\n",
      "  (1): BatchNorm1d(4352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): Linear(in_features=4352, out_features=256, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (5): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (6): ReLU()\n",
      "  (7): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (8): Linear(in_features=256, out_features=4, bias=True)\n",
      ")\n",
      "Iteration 0: 1260.04\n",
      "Iteration 50: 783.0\n",
      "Iteration 100: 923.4\n",
      "Iteration 150: 926.84\n",
      "Iteration 200: 916.84\n",
      "Iteration 250: 896.2\n",
      "Iteration 300: 920.64\n",
      "Iteration 350: 962.92\n",
      "Iteration 400: 965.88\n",
      "Iteration 450: 941.28\n",
      "Iteration 500: 908.72\n",
      "Iteration 550: 966.08\n",
      "Iteration 600: 951.24\n",
      "Iteration 650: 943.28\n",
      "Iteration 700: 957.0\n",
      "Iteration 750: 969.32\n",
      "Iteration 800: 952.2\n",
      "Iteration 850: 875.6\n",
      "Iteration 900: 879.92\n",
      "Iteration 950: 984.76\n",
      "Iteration 1000: 908.32\n"
     ]
    }
   ],
   "source": [
    "alpha_list = [1e-2, 1e-3, 1e-4, 1e-5]\n",
    "results = {}\n",
    "for alpha in alpha_list:\n",
    "    env = Game()\n",
    "    agent = Agent()\n",
    "    trainer = DQNTrainer(env, agent, alpha=alpha, epsilon_decay=0.993)\n",
    "    trainer.train(1000, eval_interval=50)\n",
    "    results[alpha] = {}\n",
    "    results[alpha]['eval'] = trainer.eval_list\n",
    "    results[alpha]['score'] = trainer.score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe3b995",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
